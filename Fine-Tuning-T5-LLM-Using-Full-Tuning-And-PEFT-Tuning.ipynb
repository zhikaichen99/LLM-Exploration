{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf214f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (23.1.2)\n",
      "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch==1.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.1) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.27.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.27.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.27.2) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.27.2) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.27.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.27.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.27.2) (2023.5.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: evaluate==0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (2023.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate==0.4.0) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (12.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->evaluate==0.4.0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate==0.4.0) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: rouge_score==0.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.4.0)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge_score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.24.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (4.65.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: loralib==0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --disable-pip-version-check\n",
    "!pip install torch==1.13.1\n",
    "!pip install torchdata==0.5.1 --quiet\n",
    "\n",
    "!pip install transformers==4.27.2\n",
    "!pip install datasets==2.11.0 --quiet\n",
    "!pip install evaluate==0.4.0\n",
    "!pip install rouge_score==0.1.2\n",
    "!pip install loralib==0.1.1\n",
    "!pip install peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7d1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1e96e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/ec2-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ceecf9f3a048d98d47511c0d1105d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883d0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre_trained model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65579ff3",
   "metadata": {},
   "source": [
    "# 2.0 Perform Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d0e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "# generate prompt\n",
    "\n",
    "prompt = \"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{}\n",
    "\n",
    "Summary:\n",
    "\"\"\".format(dialogue)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens = 200,\n",
    "    )[0],\n",
    "    skip_special_tokens = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7795c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE SUMMARY: #Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "GENERATED SUMMARY ZERO SHOT: #Person1#: Happy birthday, Brian. #Person2#: I'm so happy you're having a good time. #Person1#: Thank you, I'm sure you look great today. #Person2#: Thank you, I'm sure you look great. #Person1#: Thank you, I'm sure you look great today. #Person2#: Thank you, I'm sure you look great today. #Person1#: Thank you, I'm sure you look great today. #Person2#: Thank you, I'm sure you look great today. #Person1#: Thank you, I'm sure you look great today.\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE SUMMARY: {}\".format(summary))\n",
    "print()\n",
    "print(\"GENERATED SUMMARY ZERO SHOT: {}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc4bc5",
   "metadata": {},
   "source": [
    "# 3.0 Perform Full Fine-Tuning\n",
    "\n",
    "`Full Fine-Tuning` is the process of training a pre-trained language model on a task. When performing full fine-tuning, all the parameters of the pre-trained language model are updated during the training process.\n",
    "\n",
    "* Loading the pre-trained model\n",
    "* Task-specific dataset preparation - Prepare a labelled dataset specific to task. Dataset should be annotated with appropriate labels.\n",
    "* Model architecture modification\n",
    "* Fine-tuning process\n",
    "* Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8463502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 247577856\n",
      "Trainable Parameters: 247577856\n"
     ]
    }
   ],
   "source": [
    "# see how many parameters there are in our LLM\n",
    "all_model_params = 0\n",
    "trainable_model_params = 0\n",
    "\n",
    "for _, param in model.named_parameters():\n",
    "    all_model_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_model_params += param.numel()\n",
    "        \n",
    "print(\"Total Number of Parameters: {}\".format(all_model_params))\n",
    "print(\"Trainable Parameters: {}\".format(trainable_model_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f1612",
   "metadata": {},
   "source": [
    "## 3.1 Data Preparation\n",
    "\n",
    "Need to convert the dialogue-summary (prompt-response) pairs into explicit instructions for the LLM. Then preprocess the prompt-response dataset into tokens and pull out their `input_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae20521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    \"\"\"\n",
    "    This function converts the dialogue_summar into explicit instructions for the LLM. It then tokenizes the \n",
    "    \"\"\"\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example[\"input_ids\"] = tokenizer(prompt, padding = \"max_length\", truncation = True, return_tensors = \"pt\").input_ids\n",
    "    example[\"labels\"] = tokenizer(example[\"summary\"], padding = \"max_length\", truncation = True, return_tensors = \"pt\").input_ids\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a911c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7342a9edc524a537.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5168131c4aa8a6cf.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched = True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c892619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"id\", \"topic\", \"dialogue\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "348fca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7c7c8c8ac345425f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-96c77433143689a7.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 100 == 0, with_indices = True)\n",
    "sample_tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58004b9",
   "metadata": {},
   "source": [
    "## 3.2 Fine Tuning Model with Preprocessed Data\n",
    "\n",
    "Using built-in Hugging Face `Trainer` class. Only fine-tuning on a subset of the data to save time and resources. The performance may not be as great as we think since we're only fine-tuning on the subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8eeb473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "\n",
    "# initializing training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate = 1e-5,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay = 0.01,\n",
    "    logging_steps = 1,\n",
    "    max_steps = 1\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = sample_tokenized_dataset[\"train\"],\n",
    "    eval_dataset = sample_tokenized_dataset[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71bff47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=49.25, metrics={'train_runtime': 143.4321, 'train_samples_per_second': 0.056, 'train_steps_per_second': 0.007, 'total_flos': 5478058819584.0, 'train_loss': 49.25, 'epoch': 0.06})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run training job\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5005af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "792dce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of the AutoMmodelForSeq2SeqLM class for the instruct model\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"dialogue-summary-training-1689008306\", torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495ca1d",
   "metadata": {},
   "source": [
    "# 4.0 Model Evaluation After Full Fine Tuning\n",
    "\n",
    "## 4.1 Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53097299",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "# generate prompt\n",
    "\n",
    "prompt = \"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{}\n",
    "\n",
    "Summary:\n",
    "\"\"\".format(dialogue)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
    "\n",
    "original_model_output = model.generate(input_ids =inputs, \n",
    "                                       generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "original_model_output_decode = tokenizer.decode(original_model_output[0], skip_special_tokens = True)\n",
    "\n",
    "instruct_model_output = instruct_model.generate(input_ids = inputs,\n",
    "                                               generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "instruct_model_output_decode = tokenizer.decode(instruct_model_output[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efcc9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE SUMMARY: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "GENERATED SUMMARY ZERO SHOT: #Person1#: You'd like to upgrade your computer. #Person2: You'd like to upgrade your computer.\n",
      "\n",
      "GENERATED SUMMARY FINE-TUNED: #Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"BASELINE SUMMARY: {}\".format(summary))\n",
    "print()\n",
    "print(\"GENERATED SUMMARY ZERO SHOT: {}\".format(original_model_output_decode))\n",
    "print()\n",
    "print(\"GENERATED SUMMARY FINE-TUNED: {}\".format(instruct_model_output_decode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b5833",
   "metadata": {},
   "source": [
    "## 4.2 Quantitative Evaluation (ROUGE)\n",
    "\n",
    "`ROUGE` is a metric used to quantify the validity of summarization produced. It compares the baseline summary to the generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53b48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge evaluation\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "394a8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the outputs for the sample of the test dataset \n",
    "dialogues = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = \"\"\"\n",
    "    Summarize the following conversation.\n",
    "    \n",
    "    {}\n",
    "    \n",
    "    Summary\n",
    "    \"\"\".format(dialogue)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
    "\n",
    "    original_model_output = model.generate(input_ids =inputs, \n",
    "                                       generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "    original_model_output_decode = tokenizer.decode(original_model_output[0], skip_special_tokens = True)\n",
    "    original_model_summaries.append(original_model_output_decode)\n",
    "\n",
    "    instruct_model_output = instruct_model.generate(input_ids = inputs,\n",
    "                                               generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "    instruct_model_output_decode = tokenizer.decode(instruct_model_output[0], skip_special_tokens = True)\n",
    "    instruct_model_summaries.append(instruct_model_output_decode)\n",
    "    \n",
    "zipped_summaries = list(zip(baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = [\"Baseline\", \"Original-Model\", \"Fine-Tuned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2232b5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Original-Model</th>\n",
       "      <th>Fine-Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo is being distributed to all employees...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>This memo should go out as an intra-office mem...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Employees who use the Instant Messaging system...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>#Person1: You're finally here!</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>#Person1#: You've finally found your way home.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The person is driving to work.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorce.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1: They are getting divorced. #Person2:...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian's birthday is coming up.</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Baseline  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                      Original-Model  \\\n",
       "0  The memo is being distributed to all employees...   \n",
       "1  This memo should go out as an intra-office mem...   \n",
       "2  Employees who use the Instant Messaging system...   \n",
       "3                     #Person1: You're finally here!   \n",
       "4     #Person1#: You've finally found your way home.   \n",
       "5                     The person is driving to work.   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7                Masha and Hero are getting divorce.   \n",
       "8  #Person1: They are getting divorced. #Person2:...   \n",
       "9                     Brian's birthday is coming up.   \n",
       "\n",
       "                                          Fine-Tuned  \n",
       "0  The memo will go out to all employees by this ...  \n",
       "1  The memo will go out to all employees by this ...  \n",
       "2  The memo will go out to all employees by this ...  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: I...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2718a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models using ROUGE metric\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions = original_model_summaries,\n",
    "    references = baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references = baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d38477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2517194351542178, 'rouge2': 0.07682569526047786, 'rougeL': 0.20633697770997542, 'rougeLsum': 0.21046936832978025}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.2488908619621208, 'rouge2': 0.1104891774891775, 'rougeL': 0.2158497376018475, 'rougeLsum': 0.2190598097174996}\n"
     ]
    }
   ],
   "source": [
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f83c0",
   "metadata": {},
   "source": [
    "There is an improvement in the ROUGE score despite only fine-tuning the model a subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46905c98",
   "metadata": {},
   "source": [
    "# 5.0 Perform Parameter Efficient Fine Tuning (PEFT\n",
    "\n",
    "`Parameter Efficient Fine-Tuning` is another fine tuning method that is much more efficient than full fine tuning. Though the final results may not be as high as Full fine tuning, the method is efficient and generates comparable results.\n",
    "\n",
    "After fine tuning for a specific task, the result is that the original LLM remains unchanged and a newly-trained `LoRA Adapter` emerges. This LoRA Adapter is much smaller than the original LLM.\n",
    "\n",
    "During inference time the LoRA Adapter is combined with the original LLM to serve the inference request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e19055",
   "metadata": {},
   "source": [
    "## 5.1 Setup PEFT Model For Fine-Tuning\n",
    "\n",
    "Set up the PEFT model for fine-tuning with a new layer/parameter adapter. Using PEFT, we freeze the underlying LLM and only train the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8324e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539accd1",
   "metadata": {},
   "source": [
    "Add LoRA Adapter layers to the original LLM to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b56d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "609bb39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 251116800\n",
      "Trainable Parameters: 3538944\n"
     ]
    }
   ],
   "source": [
    "# see how many parameters there are in our LLM\n",
    "all_model_params = 0\n",
    "trainable_model_params = 0\n",
    "\n",
    "for _, param in peft_model.named_parameters():\n",
    "    all_model_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_model_params += param.numel()\n",
    "        \n",
    "print(\"Total Number of Parameters: {}\".format(all_model_params))\n",
    "print(\"Trainable Parameters: {}\".format(trainable_model_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819a32b",
   "metadata": {},
   "source": [
    "## 5.2 Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4a65db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    auto_find_batch_size = True,\n",
    "    learning_rate = 1e-3,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps = 1,\n",
    "    max_steps = 1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = peft_training_args,\n",
    "    train_dataset = sample_tokenized_dataset[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7482414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f729f",
   "metadata": {},
   "source": [
    "Prepare an instance of this model by adding an adapter to the original LLM model. Setting `is_trainable = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13cc889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# original LLM model\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# adding layer to original model\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                      './peft-dialogue-summary-checkpoint-local',\n",
    "                                      torch_dtype = torch.bfloat16,\n",
    "                                      is_trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da1e0f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 251116800\n",
      "Trainable Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# see how many parameters there are in our LLM\n",
    "all_model_params = 0\n",
    "trainable_model_params = 0\n",
    "\n",
    "for _, param in peft_model.named_parameters():\n",
    "    all_model_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_model_params += param.numel()\n",
    "        \n",
    "print(\"Total Number of Parameters: {}\".format(all_model_params))\n",
    "print(\"Trainable Parameters: {}\".format(trainable_model_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb61de",
   "metadata": {},
   "source": [
    "# 6.0 Model Evaluation After PEFT-Tuning\n",
    "\n",
    "## 6.1 Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6bd353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "# generate prompt\n",
    "\n",
    "prompt = \"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{}\n",
    "\n",
    "Summary:\n",
    "\"\"\".format(dialogue)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
    "\n",
    "original_model_output = model.generate(input_ids =inputs, \n",
    "                                       generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "original_model_output_decode = tokenizer.decode(original_model_output[0], skip_special_tokens = True)\n",
    "\n",
    "peft_model_output = peft_model.generate(input_ids = inputs,\n",
    "                                               generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "peft_model_output_decode = tokenizer.decode(peft_model_output[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47593499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE SUMMARY: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "GENERATED SUMMARY ZERO SHOT: #Pork1: Have you considered upgrading your system? #Person1: Yes, but I'm not sure what exactly I would need. #Person1: I'd like to make up my own flyers and banners for advertising. #Person1: I'd like to make up my own flyers and banners for advertising. #Person2: That would be a definite bonus. #Person1: You can do that. #Person1: I'd like to do that.\n",
      "\n",
      "GENERATED SUMMARY FINE-TUNED: #Person1#: Happy birthday, Brian. #Person2#: I'm so happy you're having a good time. #Person1#: Thank you, I'm sure you look great today. #Person2#: Thank you, I'm sure you look great. #Person1#: Thank you, I'm sure you look great today. #Person2#: Thank you, I'm sure you look great today. #Person1#: Thank you, I'm sure you look great today.\n",
      "\n",
      "GENERATED SUMMARY PEFT-TUNED: #Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"BASELINE SUMMARY: {}\".format(summary))\n",
    "print()\n",
    "print(\"GENERATED SUMMARY ZERO SHOT: {}\".format(original_model_output_decode))\n",
    "print()\n",
    "print(\"GENERATED SUMMARY FINE-TUNED: {}\".format(instruct_model_output_decode))\n",
    "print()\n",
    "print(\"GENERATED SUMMARY PEFT-TUNED: {}\".format(peft_model_output_decode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0be26",
   "metadata": {},
   "source": [
    "## 6.2 Quantitative Evaluation (ROUGE)\n",
    "\n",
    "`ROUGE` is a metric used to quantify the validity of summarization produced. It compares the baseline summary to the generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba87b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the outputs for the sample of the test dataset \n",
    "dialogues = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = \"\"\"\n",
    "    Summarize the following conversation.\n",
    "    \n",
    "    {}\n",
    "    \n",
    "    Summary\n",
    "    \"\"\".format(dialogue)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
    "\n",
    "    original_model_output = model.generate(input_ids =inputs, \n",
    "                                       generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "    original_model_output_decode = tokenizer.decode(original_model_output[0], skip_special_tokens = True)\n",
    "    original_model_summaries.append(original_model_output_decode)\n",
    "\n",
    "    instruct_model_output = instruct_model.generate(input_ids = inputs,\n",
    "                                               generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "    instruct_model_output_decode = tokenizer.decode(instruct_model_output[0], skip_special_tokens = True)\n",
    "    instruct_model_summaries.append(instruct_model_output_decode)\n",
    "    \n",
    "    peft_model_output = peft_model.generate(input_ids = inputs,\n",
    "                                               generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\n",
    "    peft_model_output_decode = tokenizer.decode(peft_model_output[0], skip_special_tokens = True)\n",
    "    peft_model_summaries.append(peft_model_output_decode)\n",
    "    \n",
    "    \n",
    "zipped_summaries = list(zip(baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = [\"Baseline\", \"Original-Model\", \"Fine-Tuned\", \"PEFT-Tuned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d62e85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Original-Model</th>\n",
       "      <th>Fine-Tuned</th>\n",
       "      <th>PEFT-Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Employees are required to use instant messaging.</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>This memo is for all employees.</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Employees who use Instant Message will receive...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The person in the first sentence is talking ab...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>Person1: I'm so glad you got stuck in traffic ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The car is causing a lot of traffic in the city.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>#Person1: Masha and Hero are getting divorced....</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting married.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1: Thank you for coming to the party. #...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Baseline  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                      Original-Model  \\\n",
       "0   Employees are required to use instant messaging.   \n",
       "1                    This memo is for all employees.   \n",
       "2  Employees who use Instant Message will receive...   \n",
       "3  The person in the first sentence is talking ab...   \n",
       "4  Person1: I'm so glad you got stuck in traffic ...   \n",
       "5   The car is causing a lot of traffic in the city.   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7  #Person1: Masha and Hero are getting divorced....   \n",
       "8                Masha and Hero are getting married.   \n",
       "9  #Person1: Thank you for coming to the party. #...   \n",
       "\n",
       "                                          Fine-Tuned  \\\n",
       "0  The memo will go out to all employees by this ...   \n",
       "1  The memo will go out to all employees by this ...   \n",
       "2  The memo will go out to all employees by this ...   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "5  The traffic jam at the Carrefour intersection ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: I...   \n",
       "\n",
       "                                          PEFT-Tuned  \n",
       "0  The memo is to be distributed to all employees...  \n",
       "1  The memo is to be distributed to all employees...  \n",
       "2  The memo is to be distributed to all employees...  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: I...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32eb310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models using ROUGE metric\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions = original_model_summaries,\n",
    "    references = baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references = baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions = peft_model_summaries,\n",
    "    references = baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54f39edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2032152477543929, 'rouge2': 0.07742309097552065, 'rougeL': 0.18894173166390682, 'rougeLsum': 0.1894183320685648}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.2488908619621208, 'rouge2': 0.1104891774891775, 'rougeL': 0.2158497376018475, 'rougeLsum': 0.2190598097174996}\n",
      "PEFT MODEL: \n",
      "{'rouge1': 0.2517460317460318, 'rouge2': 0.10979007465963989, 'rougeL': 0.2128638229725186, 'rougeLsum': 0.21601610748349878}\n"
     ]
    }
   ],
   "source": [
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print(\"PEFT MODEL: \")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4e23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
